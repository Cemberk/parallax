// NVBit binary instrumentation tool for prlx.
//
// Usage:
//   LD_PRELOAD=./build/lib/nvbit_tool/libprlx_nvbit.so
//   PRLX_TRACE=trace.prlx ./my_cuda_app
//
// Architecture follows NVBit 1.7.7+ mem_trace reference:
//   - Per-context state (CTXstate) for multi-GPU / multi-context support
//   - channel_dev and grid_launch_id passed to inject functions as arguments
//   - Re-entrancy protection via skip_callback_flag
//   - flush_channel kernel for proper channel drain
//   - All launch API variants including CUDA graphs

#include <pthread.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

#include <atomic>
#include <mutex>
#include <string>
#include <unordered_map>
#include <unordered_set>
#include <vector>

#include "nvbit.h"
#include "nvbit_tool.h"
#include "utils/channel.hpp"

#include "common.h"
#include "site_table.h"
#include "trace_writer.h"

// Embedded flush_channel fatbin (generated by bin2c at build time)
#include "flush_channel.c"

// ---- Configuration (from environment variables) ----
static struct {
    std::string trace_path = "trace.prlx";
    std::string session_dir;
    std::string sites_path = "prlx-sites.json";
    std::string filter_pattern;
    uint32_t buffer_size = PRLX_NVBIT_DEFAULT_BUFFER_SIZE;
    uint32_t sample_rate = 1;
    bool enabled = true;
    bool compress = false;
    bool instrument_stores = false;
} g_config;

// ---- Session manifest entry ----
struct NvbitSessionEntry {
    std::string kernel;
    std::string file;
    uint32_t launch;
    uint32_t grid[3];
    uint32_t block[3];
};

// ---- Receiver thread state machine ----
enum class RecvThreadState { INIT, WORKING, STOP, FINISHED };

// ---- Per-context state (following NVBit mem_trace pattern) ----
// Each CUDA context gets its own channel, receiver thread, and flush kernel.
// This is required for multi-GPU and multi-context applications.
struct CTXstate {
    int id;
    ChannelDev* channel_dev;
    ChannelHost channel_host;
    CUmodule tool_module;
    CUfunction flush_channel_func;
    volatile RecvThreadState recv_thread_done = RecvThreadState::INIT;
    bool need_sync = false;
};

// ---- Global state ----
static prlx::NvbitSiteTable g_site_table;
static prlx::TraceWriter* g_writer = nullptr;
static std::mutex g_writer_mutex;
static uint64_t g_grid_launch_id = 0;
static uint32_t g_launch_count = 0;
static uint32_t g_session_launch_idx = 0;
static std::vector<NvbitSessionEntry> g_session_entries;
static std::unordered_set<CUfunction> g_already_instrumented;

// Per-context state map
static std::unordered_map<CUcontext, CTXstate*> g_ctx_state_map;

// Mutexes (recursive, matching NVBit reference pattern)
static pthread_mutex_t g_mutex;
static pthread_mutex_t g_cuda_event_mutex;

// Re-entrancy guard: prevents our own CUDA calls from triggering callbacks
static bool g_skip_callback_flag = false;

// Statistics counters
static std::atomic<uint64_t> g_stat_events_received{0};
static std::atomic<uint64_t> g_stat_events_written{0};

#define CHANNEL_SIZE (1l << 20)

// ---- SASS opcode classification ----

static bool is_branch_opcode(const char* opcode) {
    return (strcmp(opcode, "BRA") == 0 ||
            strcmp(opcode, "BRX") == 0 ||
            strcmp(opcode, "JMP") == 0 ||
            strcmp(opcode, "JMX") == 0 ||
            strcmp(opcode, "BREAK") == 0 ||
            strcmp(opcode, "CONT") == 0 ||
            strcmp(opcode, "BSYNC") == 0);
}

static bool is_shmem_store_opcode(const char* opcode) {
    return (strcmp(opcode, "STS") == 0 ||
            strncmp(opcode, "STS.", 4) == 0);
}

static bool is_atomic_opcode(const char* opcode) {
    return (strcmp(opcode, "ATOM") == 0 ||
            strncmp(opcode, "ATOM.", 5) == 0 ||
            strcmp(opcode, "ATOMS") == 0 ||
            strncmp(opcode, "ATOMS.", 6) == 0 ||
            strcmp(opcode, "RED") == 0 ||
            strncmp(opcode, "RED.", 4) == 0);
}

static bool is_global_store_opcode(const char* opcode) {
    return (strcmp(opcode, "STG") == 0 ||
            strncmp(opcode, "STG.", 4) == 0);
}

static bool is_func_exit_opcode(const char* opcode) {
    return (strcmp(opcode, "RET") == 0 ||
            strcmp(opcode, "EXIT") == 0);
}

// ---- Kernel name filter ----
static bool matches_filter(const std::string& kernel_name) {
    if (g_config.filter_pattern.empty()) return true;
    return kernel_name.find(g_config.filter_pattern) != std::string::npos;
}

// ---- Receiver thread ----
// Per-context pthreads callback — consumes channel events and feeds to trace writer.

static void* recv_thread_fun(void* args) {
    CUcontext ctx = (CUcontext)args;

    pthread_mutex_lock(&g_mutex);
    auto it = g_ctx_state_map.find(ctx);
    if (it == g_ctx_state_map.end()) {
        fprintf(stderr, "[prlx-nvbit] ERROR: recv_thread_fun: context not found in state map\n");
        pthread_mutex_unlock(&g_mutex);
        return nullptr;
    }
    CTXstate* ctx_state = it->second;
    ChannelHost* ch_host = &ctx_state->channel_host;
    pthread_mutex_unlock(&g_mutex);

    char* recv_buffer = (char*)malloc(CHANNEL_SIZE);
    if (!recv_buffer) {
        fprintf(stderr, "[prlx-nvbit] ERROR: recv_thread_fun: malloc(%ld) failed\n",
                (long)CHANNEL_SIZE);
        ctx_state->recv_thread_done = RecvThreadState::FINISHED;
        return nullptr;
    }

    while (ctx_state->recv_thread_done == RecvThreadState::WORKING) {
        uint32_t num_recv_bytes = ch_host->recv(recv_buffer, CHANNEL_SIZE);

        if (num_recv_bytes > 0) {
            uint32_t num_events = num_recv_bytes / sizeof(prlx_channel_event_t);
            g_stat_events_received.fetch_add(num_events, std::memory_order_relaxed);

            std::lock_guard<std::mutex> lock(g_writer_mutex);
            if (g_writer) {
                const prlx_channel_event_t* events =
                    reinterpret_cast<const prlx_channel_event_t*>(recv_buffer);
                for (uint32_t i = 0; i < num_events; i++) {
                    prlx_channel_event_t translated = events[i];
                    translated.site_id = g_site_table.lookup(events[i].site_id);
                    g_writer->add_event(translated);
                }
                g_stat_events_written.fetch_add(num_events, std::memory_order_relaxed);
            }
        } else {
            usleep(100);
        }
    }

    free(recv_buffer);
    ctx_state->recv_thread_done = RecvThreadState::FINISHED;
    return nullptr;
}

// ---- Instrumentation ----
// Instruments a single function: walks SASS instructions and inserts inject calls.
// Each inject function receives grid_launch_id via nvbit_add_call_arg_launch_val64
// (set per-launch with nvbit_set_at_launch) and channel_dev pointer via
// nvbit_add_call_arg_const_val64 (set once at instrumentation time).

static void instrument_single_function(CUcontext ctx, CUfunction func,
                                        CTXstate* ctx_state) {
    const char* func_name = nvbit_get_func_name(ctx, func, true);
    if (!matches_filter(func_name)) return;

    const std::vector<Instr*>& instrs = nvbit_get_instrs(ctx, func);
    if (instrs.empty()) return;

    bool first_instr = true;

    for (auto* instr : instrs) {
        const char* opcode = instr->getOpcode();
        uint32_t sass_pc = instr->getOffset();

        // Query per-instruction source line info from debug info
        char* nvbit_file = nullptr;
        char* nvbit_dir = nullptr;
        uint32_t nvbit_line = 0;
        const char* filename = "";
        uint32_t line = 0;
        if (nvbit_get_line_info(ctx, func, sass_pc, &nvbit_file, &nvbit_dir, &nvbit_line)) {
            if (nvbit_file) filename = nvbit_file;
            line = nvbit_line;
        }

        if (first_instr) {
            g_site_table.register_site(
                sass_pc, PRLX_EVENT_FUNC_ENTRY, filename, func_name, line);

            nvbit_insert_call(instr, "prlx_instr_func_entry", IPOINT_BEFORE);
            nvbit_add_call_arg_guard_pred_val(instr);
            nvbit_add_call_arg_const_val32(instr, sass_pc);
            nvbit_add_call_arg_const_val32(instr, g_config.sample_rate);
            nvbit_add_call_arg_launch_val64(instr, 0);
            nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
            first_instr = false;
        }

        if (is_branch_opcode(opcode)) {
            g_site_table.register_site(sass_pc, PRLX_EVENT_BRANCH, filename, func_name, line);

            nvbit_insert_call(instr, "prlx_instr_branch", IPOINT_BEFORE);
            nvbit_add_call_arg_guard_pred_val(instr);
            nvbit_add_call_arg_guard_pred_val(instr);  // branch direction = guard predicate
            nvbit_add_call_arg_const_val32(instr, sass_pc);
            nvbit_add_call_arg_const_val32(instr, g_config.sample_rate);
            nvbit_add_call_arg_launch_val64(instr, 0);
            nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
        }
        else if (is_shmem_store_opcode(opcode)) {
            g_site_table.register_site(sass_pc, PRLX_EVENT_SHMEM_STORE, filename, func_name, line);

            nvbit_insert_call(instr, "prlx_instr_shmem_store", IPOINT_BEFORE);
            nvbit_add_call_arg_guard_pred_val(instr);
            nvbit_add_call_arg_const_val32(instr, sass_pc);
            nvbit_add_call_arg_mref_addr64(instr, 0);
            nvbit_add_call_arg_const_val32(instr, g_config.sample_rate);
            nvbit_add_call_arg_launch_val64(instr, 0);
            nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
        }
        else if (g_config.instrument_stores && is_global_store_opcode(opcode)) {
            g_site_table.register_site(sass_pc, PRLX_EVENT_GLOBAL_STORE, filename, func_name, line);

            nvbit_insert_call(instr, "prlx_instr_global_store", IPOINT_BEFORE);
            nvbit_add_call_arg_guard_pred_val(instr);
            nvbit_add_call_arg_const_val32(instr, sass_pc);
            nvbit_add_call_arg_mref_addr64(instr, 0);
            nvbit_add_call_arg_const_val32(instr, g_config.sample_rate);
            nvbit_add_call_arg_launch_val64(instr, 0);
            nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
        }
        else if (is_atomic_opcode(opcode)) {
            g_site_table.register_site(sass_pc, PRLX_EVENT_ATOMIC, filename, func_name, line);

            nvbit_insert_call(instr, "prlx_instr_atomic", IPOINT_BEFORE);
            nvbit_add_call_arg_guard_pred_val(instr);
            nvbit_add_call_arg_const_val32(instr, sass_pc);
            nvbit_add_call_arg_mref_addr64(instr, 0);
            nvbit_add_call_arg_const_val32(instr, g_config.sample_rate);
            nvbit_add_call_arg_launch_val64(instr, 0);
            nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
        }
        else if (is_func_exit_opcode(opcode)) {
            g_site_table.register_site(sass_pc, PRLX_EVENT_FUNC_EXIT, filename, func_name, line);

            nvbit_insert_call(instr, "prlx_instr_func_exit", IPOINT_BEFORE);
            nvbit_add_call_arg_guard_pred_val(instr);
            nvbit_add_call_arg_const_val32(instr, sass_pc);
            nvbit_add_call_arg_const_val32(instr, g_config.sample_rate);
            nvbit_add_call_arg_launch_val64(instr, 0);
            nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
        }
    }
}

// Instrument a kernel and all its related device functions.
static void instrument_function_if_needed(CUcontext ctx, CUfunction func) {
    auto it = g_ctx_state_map.find(ctx);
    if (it == g_ctx_state_map.end()) {
        fprintf(stderr, "[prlx-nvbit] ERROR: instrument_function_if_needed: context not found\n");
        return;
    }
    CTXstate* ctx_state = it->second;

    // Get related device functions that the kernel may call
    std::vector<CUfunction> related_functions =
        nvbit_get_related_functions(ctx, func);
    related_functions.push_back(func);

    for (auto f : related_functions) {
        if (!g_already_instrumented.insert(f).second) {
            continue;  // Already instrumented
        }
        instrument_single_function(ctx, f, ctx_state);
    }
}

// ---- Session manifest writer ----
static void write_session_manifest() {
    if (g_config.session_dir.empty() || g_session_entries.empty()) return;

    std::string manifest_path = g_config.session_dir + "/session.json";
    FILE* f = fopen(manifest_path.c_str(), "w");
    if (!f) {
        fprintf(stderr, "[prlx-nvbit] Failed to write session manifest to %s\n",
                manifest_path.c_str());
        return;
    }

    fprintf(f, "[\n");
    for (size_t i = 0; i < g_session_entries.size(); i++) {
        const auto& e = g_session_entries[i];
        std::string escaped_kernel;
        for (char c : e.kernel) {
            if (c == '\\') escaped_kernel += "\\\\";
            else if (c == '"') escaped_kernel += "\\\"";
            else escaped_kernel += c;
        }
        fprintf(f,
            "  {\"kernel\": \"%s\", \"launch\": %u, "
            "\"grid\": [%u, %u, %u], \"block\": [%u, %u, %u], "
            "\"file\": \"launch_%u.prlx\"}%s\n",
            escaped_kernel.c_str(), e.launch,
            e.grid[0], e.grid[1], e.grid[2],
            e.block[0], e.block[1], e.block[2],
            e.launch,
            (i + 1 < g_session_entries.size()) ? "," : "");
    }
    fprintf(f, "]\n");
    fclose(f);

    fprintf(stderr, "[prlx-nvbit] Wrote session manifest: %s (%zu launches)\n",
            manifest_path.c_str(), g_session_entries.size());
}

// ---- Context state initialization ----
static void init_context_state(CUcontext ctx) {
    CTXstate* ctx_state = g_ctx_state_map[ctx];
    ctx_state->recv_thread_done = RecvThreadState::WORKING;
    cudaMallocManaged(&ctx_state->channel_dev, sizeof(ChannelDev));
    ctx_state->channel_host.init((int)g_ctx_state_map.size() - 1, CHANNEL_SIZE,
                                  ctx_state->channel_dev, recv_thread_fun, ctx);
    nvbit_set_tool_pthread(ctx_state->channel_host.get_thread());
}

// ---- Common kernel launch handling ----

static void enter_kernel_launch(CUcontext ctx, CUfunction func,
                                 uint32_t gridDimX, uint32_t gridDimY, uint32_t gridDimZ,
                                 uint32_t blockDimX, uint32_t blockDimY, uint32_t blockDimZ,
                                 bool stream_capture, bool build_graph) {
    auto it = g_ctx_state_map.find(ctx);
    if (it == g_ctx_state_map.end()) {
        fprintf(stderr, "[prlx-nvbit] ERROR: enter_kernel_launch: context not found\n");
        return;
    }
    CTXstate* ctx_state = it->second;

    // Instrument the function and all its device function callees
    instrument_function_if_needed(ctx, func);

    // During stream capture or graph build, no kernel actually runs yet.
    // Instrumentation is inserted but launch arguments are set later
    // at graph node execution time (nvbit_at_graph_node_launch).
    if (!stream_capture && !build_graph) {
        const char* kernel_name = nvbit_get_func_name(ctx, func, true);

        CUdevice cu_dev;
        cuCtxGetDevice(&cu_dev);
        int major = 0, minor = 0;
        cuDeviceGetAttribute(&major, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, cu_dev);
        cuDeviceGetAttribute(&minor, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, cu_dev);
        uint32_t arch = major * 10 + minor;

        {
            std::lock_guard<std::mutex> lock(g_writer_mutex);
            if (g_writer) {
                g_writer->set_kernel_info(
                    kernel_name,
                    gridDimX, gridDimY, gridDimZ,
                    blockDimX, blockDimY, blockDimZ,
                    arch);
            }
        }

        // Set grid_launch_id for inject functions
        nvbit_set_at_launch(ctx, func, (uint64_t)g_grid_launch_id);
        g_grid_launch_id++;
        g_launch_count++;
    }

    // Enable instrumented code to run (required by NVBit)
    nvbit_enable_instrumented(ctx, func, true);
}

static void leave_kernel_launch(CUcontext ctx, CUfunction func) {
    // Session mode: write per-kernel trace files after each launch
    if (!g_config.session_dir.empty()) {
        const char* kernel_name = nvbit_get_func_name(ctx, func, true);
        uint32_t this_launch = g_session_launch_idx++;

        std::lock_guard<std::mutex> lock(g_writer_mutex);
        if (g_writer && g_writer->total_events() > 0) {
            std::string kernel_file = g_config.session_dir + "/launch_" +
                std::to_string(this_launch) + ".prlx";
            g_writer->write(kernel_file, g_config.compress);

            NvbitSessionEntry entry;
            entry.kernel = kernel_name ? kernel_name : "unknown";
            entry.file = kernel_file;
            entry.launch = this_launch;
            // Grid/block dims not available in is_exit for all launch types,
            // but the writer already captured them in set_kernel_info.
            entry.grid[0] = entry.grid[1] = entry.grid[2] = 0;
            entry.block[0] = entry.block[1] = entry.block[2] = 0;
            g_session_entries.push_back(entry);

            g_writer->clear();
        }
        if (g_site_table.size() > 0) {
            g_site_table.export_json(g_config.sites_path);
        }
    }
}

// ---- NVBit callbacks ----

void nvbit_at_init() {
    setenv("CUDA_MANAGED_FORCE_DEVICE_ALLOC", "1", 1);

    // Initialize recursive mutexes (following NVBit reference pattern)
    pthread_mutexattr_t attr;
    pthread_mutexattr_init(&attr);
    pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE);
    pthread_mutex_init(&g_mutex, &attr);
    pthread_mutex_init(&g_cuda_event_mutex, &attr);

    if (const char* v = getenv("PRLX_ENABLED")) {
        g_config.enabled = (atoi(v) != 0);
    }
    if (!g_config.enabled) {
        fprintf(stderr, "[prlx-nvbit] Disabled via PRLX_ENABLED=0\n");
        return;
    }

    if (const char* v = getenv("PRLX_TRACE")) g_config.trace_path = v;
    if (const char* v = getenv("PRLX_SESSION")) g_config.session_dir = v;
    if (const char* v = getenv("PRLX_SITES")) g_config.sites_path = v;
    if (const char* v = getenv("PRLX_FILTER")) g_config.filter_pattern = v;
    if (const char* v = getenv("PRLX_BUFFER_SIZE")) g_config.buffer_size = atoi(v);
    if (const char* v = getenv("PRLX_SAMPLE_RATE")) g_config.sample_rate = atoi(v);
    if (const char* v = getenv("PRLX_COMPRESS")) g_config.compress = (atoi(v) != 0);
    if (const char* v = getenv("PRLX_INSTRUMENT_STORES")) g_config.instrument_stores = (atoi(v) != 0);

    fprintf(stderr, "[prlx-nvbit] Initialized: trace=%s, buffer=%u, filter='%s'\n",
            g_config.trace_path.c_str(),
            g_config.buffer_size,
            g_config.filter_pattern.c_str());

    g_writer = new prlx::TraceWriter(g_config.buffer_size);
    if (g_config.sample_rate > 1) {
        g_writer->set_sample_rate(g_config.sample_rate);
    }
}

void nvbit_at_ctx_init(CUcontext ctx) {
    pthread_mutex_lock(&g_mutex);
    if (!g_config.enabled) {
        pthread_mutex_unlock(&g_mutex);
        return;
    }
    // NOTE: No CUDA memory allocation here — NVBit docs say it will deadlock.
    // Channel allocation happens in nvbit_tool_init() below.

    if (g_ctx_state_map.find(ctx) != g_ctx_state_map.end()) {
        fprintf(stderr, "[prlx-nvbit] WARNING: nvbit_at_ctx_init: context already registered, skipping\n");
        pthread_mutex_unlock(&g_mutex);
        return;
    }
    CTXstate* ctx_state = new CTXstate;
    ctx_state->id = (int)g_ctx_state_map.size();
    g_ctx_state_map[ctx] = ctx_state;

    // Load the flush_channel tool module (no CUDA allocs, just module loading)
    nvbit_load_tool_module(ctx, (const void*)flush_channel_bin, &ctx_state->tool_module);
    nvbit_find_function_by_name(ctx, ctx_state->tool_module, "flush_channel",
                                &ctx_state->flush_channel_func);
    pthread_mutex_unlock(&g_mutex);
}

// Called before first kernel launch — safe to do CUDA allocations here.
void nvbit_tool_init(CUcontext ctx) {
    pthread_mutex_lock(&g_mutex);
    if (!g_config.enabled) {
        pthread_mutex_unlock(&g_mutex);
        return;
    }
    if (g_ctx_state_map.find(ctx) == g_ctx_state_map.end()) {
        fprintf(stderr, "[prlx-nvbit] ERROR: nvbit_tool_init: context not registered\n");
        pthread_mutex_unlock(&g_mutex);
        return;
    }
    init_context_state(ctx);
    pthread_mutex_unlock(&g_mutex);
}

void nvbit_at_ctx_term(CUcontext ctx) {
    pthread_mutex_lock(&g_mutex);
    if (!g_config.enabled) {
        pthread_mutex_unlock(&g_mutex);
        return;
    }

    g_skip_callback_flag = true;

    auto term_it = g_ctx_state_map.find(ctx);
    if (term_it == g_ctx_state_map.end()) {
        fprintf(stderr, "[prlx-nvbit] ERROR: nvbit_at_ctx_term: context not found\n");
        g_skip_callback_flag = false;
        pthread_mutex_unlock(&g_mutex);
        return;
    }
    CTXstate* ctx_state = term_it->second;

    // Flush channel if any kernels were launched in this context
    if (ctx_state->need_sync) {
        void* args[] = {&ctx_state->channel_dev};
        nvbit_launch_kernel(ctx, ctx_state->flush_channel_func,
                            1, 1, 1, 1, 1, 1, 0, nullptr, args, nullptr);
        cudaDeviceSynchronize();
        cudaError_t flush_err = cudaGetLastError();
        if (flush_err != cudaSuccess) {
            fprintf(stderr, "[prlx-nvbit] WARNING: flush_channel CUDA error: %s\n",
                    cudaGetErrorString(flush_err));
        }
    }

    // Signal receiver thread to stop and wait for drain
    if (ctx_state->recv_thread_done != RecvThreadState::INIT) {
        ctx_state->recv_thread_done = RecvThreadState::STOP;
        while (ctx_state->recv_thread_done != RecvThreadState::FINISHED)
            ;
    }

    ctx_state->channel_host.destroy(false);

    // Collect stats and write trace
    size_t overflow_count = 0;
    {
        std::lock_guard<std::mutex> lock(g_writer_mutex);
        if (g_writer) {
            overflow_count = g_writer->total_overflow();
            if (g_writer->total_events() > 0) {
                g_writer->write(g_config.trace_path, g_config.compress);
            }
        }
    }

    if (g_site_table.size() > 0) {
        g_site_table.export_json(g_config.sites_path);
    }

    write_session_manifest();

    delete g_writer;
    g_writer = nullptr;

    cudaFree(ctx_state->channel_dev);

    fprintf(stderr, "[prlx-nvbit] Shutdown: %u launches instrumented, %zu sites\n",
            g_launch_count, g_site_table.size());
    fprintf(stderr, "[prlx-nvbit] Stats: %lu events received, %lu events written, %zu overflows\n",
            g_stat_events_received.load(std::memory_order_relaxed),
            g_stat_events_written.load(std::memory_order_relaxed),
            overflow_count);

    g_skip_callback_flag = false;
    delete ctx_state;
    pthread_mutex_unlock(&g_mutex);
}

void nvbit_at_cuda_event(
    CUcontext ctx,
    int is_exit,
    nvbit_api_cuda_t cbid,
    const char* name,
    void* params,
    CUresult* pStatus
) {
    pthread_mutex_lock(&g_cuda_event_mutex);

    // Re-entrancy guard: skip if we're inside our own CUDA calls
    if (!g_config.enabled || g_skip_callback_flag) {
        pthread_mutex_unlock(&g_cuda_event_mutex);
        return;
    }
    g_skip_callback_flag = true;

    CTXstate* ctx_state = g_ctx_state_map[ctx];

    switch (cbid) {
        // Legacy launch APIs (no stream parameter, no graph involvement)
        case API_CUDA_cuLaunch:
        case API_CUDA_cuLaunchGrid: {
            cuLaunch_params* p = (cuLaunch_params*)params;
            if (!is_exit) {
                ctx_state->need_sync = true;
                enter_kernel_launch(ctx, p->f, 0, 0, 0, 0, 0, 0, false, false);
            } else {
                leave_kernel_launch(ctx, p->f);
            }
        } break;

        // Modern launch APIs with stream (may involve CUDA graphs)
        case API_CUDA_cuLaunchKernel_ptsz:
        case API_CUDA_cuLaunchKernel:
        case API_CUDA_cuLaunchCooperativeKernel:
        case API_CUDA_cuLaunchCooperativeKernel_ptsz:
        case API_CUDA_cuLaunchGridAsync: {
            cuLaunchKernel_params* p = (cuLaunchKernel_params*)params;

            CUstream hStream = p->hStream;
            cudaStreamCaptureStatus streamStatus;
            CUDA_SAFECALL(cudaStreamIsCapturing(hStream, &streamStatus));

            if (!is_exit) {
                bool stream_capture = (streamStatus == cudaStreamCaptureStatusActive);
                ctx_state->need_sync = true;
                enter_kernel_launch(ctx, p->f,
                                     p->gridDimX, p->gridDimY, p->gridDimZ,
                                     p->blockDimX, p->blockDimY, p->blockDimZ,
                                     stream_capture, false);
            } else {
                if (streamStatus != cudaStreamCaptureStatusActive) {
                    leave_kernel_launch(ctx, p->f);
                }
            }
        } break;

        // Extended launch API (CUDA 12.x+)
        case API_CUDA_cuLaunchKernelEx:
        case API_CUDA_cuLaunchKernelEx_ptsz: {
            cuLaunchKernelEx_params* p = (cuLaunchKernelEx_params*)params;

            CUstream hStream = p->config->hStream;
            cudaStreamCaptureStatus streamStatus;
            CUDA_SAFECALL(cudaStreamIsCapturing(hStream, &streamStatus));

            if (!is_exit) {
                bool stream_capture = (streamStatus == cudaStreamCaptureStatusActive);
                ctx_state->need_sync = true;
                enter_kernel_launch(ctx, p->f,
                                     p->config->gridDimX, p->config->gridDimY, p->config->gridDimZ,
                                     p->config->blockDimX, p->config->blockDimY, p->config->blockDimZ,
                                     stream_capture, false);
            } else {
                if (streamStatus != cudaStreamCaptureStatusActive) {
                    leave_kernel_launch(ctx, p->f);
                }
            }
        } break;

        // CUDA Graph: manual kernel node addition
        case API_CUDA_cuGraphAddKernelNode: {
            cuGraphAddKernelNode_params* p = (cuGraphAddKernelNode_params*)params;
            CUfunction func = p->nodeParams->func;
            if (!is_exit) {
                ctx_state->need_sync = true;
                enter_kernel_launch(ctx, func,
                                     p->nodeParams->gridDimX, p->nodeParams->gridDimY, p->nodeParams->gridDimZ,
                                     p->nodeParams->blockDimX, p->nodeParams->blockDimY, p->nodeParams->blockDimZ,
                                     false, true);  // build_graph = true
            }
        } break;

        default:
            break;
    }

    g_skip_callback_flag = false;
    pthread_mutex_unlock(&g_cuda_event_mutex);
}

// CUDA Graph: called when a graph node actually executes.
// Sets the per-launch argument and captures kernel info at execution time.
void nvbit_at_graph_node_launch(CUcontext ctx, CUfunction func,
                                 CUstream stream, uint64_t launch_handle) {
    const char* kernel_name = nvbit_get_func_name(ctx, func);

    pthread_mutex_lock(&g_mutex);

    CUdevice cu_dev;
    cuCtxGetDevice(&cu_dev);
    int major = 0, minor = 0;
    cuDeviceGetAttribute(&major, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, cu_dev);
    cuDeviceGetAttribute(&minor, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, cu_dev);
    uint32_t arch = major * 10 + minor;

    func_config_t config = {0};
    nvbit_get_func_config(ctx, func, &config);

    {
        std::lock_guard<std::mutex> lock(g_writer_mutex);
        if (g_writer) {
            g_writer->set_kernel_info(
                kernel_name,
                config.gridDimX, config.gridDimY, config.gridDimZ,
                config.blockDimX, config.blockDimY, config.blockDimZ,
                arch);
        }
    }

    // Set launch argument with stream and launch_handle for graph execution
    nvbit_set_at_launch(ctx, func, (uint64_t)g_grid_launch_id, stream, launch_handle);
    g_grid_launch_id++;
    g_launch_count++;

    pthread_mutex_unlock(&g_mutex);
}
